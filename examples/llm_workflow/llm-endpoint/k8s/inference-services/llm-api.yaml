apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llm-api
  namespace: kubeflow-user-example-com
  labels:
    app: llm-api
spec:
  predictor:
    containers:
    - name: llm-api-container
      image: 2cassa2/text-generation-api:0.0.1
      # image: 2cassa2/search-agent-api:0.0.1
      ports:
      - containerPort: 5000
      env:
        - name: MODEL_NAME
          valueFrom:
            configMapKeyRef:
              name: llm-api
              key: model_name
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: llm-api-secret 
              key: hf_token
        - name: LANGCHAIN_TRACING_V2
          valueFrom:
            secretKeyRef:
              name: llm-api-secret 
              key: langchain_tracing_v2
        - name: LANGCHAIN_ENDPOINT
          valueFrom:
            secretKeyRef:
              name: llm-api-secret 
              key: langchain_endpoint
        - name: LANGCHAIN_API_KEY
          valueFrom:
            secretKeyRef:
              name: llm-api-secret 
              key: langchain_api_key
        - name: LANGCHAIN_PROJECT
          valueFrom:
            secretKeyRef:
              name: llm-api-secret 
              key: langchain_project
        - name: TAVILY_API_KEY 
          valueFrom:
            secretKeyRef:
              name: llm-api-secret 
              key: tavily_api_key
          